{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Text Analysis\n",
    "An explanation this assignment could be found in the .pdf explanation document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Materials to review for this assignment\n",
    "<h4>From Moodle:</h4> \n",
    "<h5><u>Review the notebooks regarding the following python topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Working with strings</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Text Analysis</b> (tutorial notebook)<br/>\n",
    "&#x2714; <b>Hebrew text analysis tools (tokenizer, wordnet)</b> (moodle example)<br/>\n",
    "&#x2714; <b>(brief review) All previous notebooks</b><br/>\n",
    "</div> \n",
    "<h5><u>Review the presentations regarding the following topics</u>:</h5>\n",
    "<div class=\"alert alert-info\">\n",
    "&#x2714; <b>Text Analysis</b> (lecture presentation)<br/>\n",
    "&#x2714; <b>(brief review) All other presentations</b><br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details Student 1: Ortal Salman | ortalsn@gmail.com\n",
    "\n",
    "# Details Student 2: Null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preceding Step - import modules (packages)\n",
    "This step is necessary in order to use external modules (packages). <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# ------------- visualizations:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics, pipeline, model_selection, feature_extraction \n",
    "from sklearn import naive_bayes, linear_model, svm, neural_network, neighbors, tree\n",
    "from sklearn import decomposition, cluster\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ----------------- output and visualizations: \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "# show several prints in one cell. This will allow us to condence every trick in one cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text analysis and String manipulation imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# --------- Text analysis and Hebrew text analysis imports:\n",
    "# vectorizers:\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regular expressions:\n",
    "import re\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - WordNet (for Hebrew)\n",
    "Note: the WordNet is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install Wordnet (for Hebrew) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net installation:\n",
    "\n",
    "# unmark if you want to use and need to install\n",
    "# !pip install wn\n",
    "# !python -m wn download omw-he:1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word net import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Hebrew text analysis - hebrew_tokenizer (Tokenizer for Hebrew)\n",
    "Note: the hebrew_tokenizer is not a must"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Only if you didn't install hebrew_tokenizer use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer installation:\n",
    "\n",
    "# unmark if you want to use and need to install:\n",
    "# !pip install hebrew_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebrew tokenizer import:\n",
    "\n",
    "# unmark if you want to use:\n",
    "# import hebrew_tokenizer as ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input files\n",
    "Reading input files for train annotated corpus (raw text data) corpus and for the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'annotated_corpus_for_train.csv'\n",
    "test_filename  = 'corpus_for_test.csv'\n",
    "df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8')\n",
    "df_test  = pd.read_csv(test_filename, index_col=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new df (the same as we uploaded) so that we have a backup:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = pd.read_csv(train_filename, index_col=None, encoding='utf-8') #train set\n",
    "new_df_test  = pd.read_csv(test_filename, index_col=None, encoding='utf-8') #test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>×›×©×—×‘×¨ ×”×–××™×Ÿ ××•×ª×™ ×œ×—×•×œ, ×œ× ×‘×××ª ×—×©×‘×ª×™ ×©×–×” ×™×§×¨×”,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>×œ×¤× ×™ ×©×”×ª×’×™×™×¡×ª×™ ×œ×¦×‘× ×¢×©×™×ª×™ ×›×œ ×× ×™ ××™×•× ×™× ×œ×™×—×™×“×•...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>×××– ×©×”×ª×—×™×œ×• ×”×œ×™××•×“×™× ×—×œ×•××• ×©×œ ×›×œ ×¡×˜×•×“× ×˜ ×–×” ×”×¤× ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>×›×©×”×™×™×ª×™ ×™×œ×“, ××˜×•×¡×™× ×”×™×” ×”×“×‘×¨ ×©×”×›×™ ×¨×™×ª×§ ××•×ª×™. ×‘...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>â€×”×™×™×ª×™ ××“×¨×™×›×” ×‘×›×¤×¨ × ×•×¢×¨ ×•××ª×× ×”×›×¤×¨ ×”×™×™× ×• ×¦×¨×™×›×™...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>×œ×¤× ×™ ×›3 ×—×•×“×©×™× ×˜×¡×ª×™ ×œ×¨×•×× ×œ××©×š ×©×‘×•×¢. ×˜×¡×ª×™ ×‘××˜×•...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>×× ×™ ×›×‘×¨ ×©× ×ª×™×™× × ×©×•×™ ×•×”×©× ×” ×× ×™ ×•××™×©×ª×™ ×¡×•×£ ×¡×•×£ ×™...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>×”×©× ×” ×”×ª×—×œ× ×• ×©×™×¤×•×¥ ×‘×“×™×¨×” ×©×œ× ×• ×‘×ª×œ ××‘×™×‘. ×”×“×™×¨×” ×”...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  ×›×©×—×‘×¨ ×”×–××™×Ÿ ××•×ª×™ ×œ×—×•×œ, ×œ× ×‘×××ª ×—×©×‘×ª×™ ×©×–×” ×™×§×¨×”,...      m\n",
       "1  ×œ×¤× ×™ ×©×”×ª×’×™×™×¡×ª×™ ×œ×¦×‘× ×¢×©×™×ª×™ ×›×œ ×× ×™ ××™×•× ×™× ×œ×™×—×™×“×•...      m\n",
       "2  ×××– ×©×”×ª×—×™×œ×• ×”×œ×™××•×“×™× ×—×œ×•××• ×©×œ ×›×œ ×¡×˜×•×“× ×˜ ×–×” ×”×¤× ...      f\n",
       "3  ×›×©×”×™×™×ª×™ ×™×œ×“, ××˜×•×¡×™× ×”×™×” ×”×“×‘×¨ ×©×”×›×™ ×¨×™×ª×§ ××•×ª×™. ×‘...      m\n",
       "4  â€×”×™×™×ª×™ ××“×¨×™×›×” ×‘×›×¤×¨ × ×•×¢×¨ ×•××ª×× ×”×›×¤×¨ ×”×™×™× ×• ×¦×¨×™×›×™...      f\n",
       "5  ×œ×¤× ×™ ×›3 ×—×•×“×©×™× ×˜×¡×ª×™ ×œ×¨×•×× ×œ××©×š ×©×‘×•×¢. ×˜×¡×ª×™ ×‘××˜×•...      f\n",
       "6  ×× ×™ ×›×‘×¨ ×©× ×ª×™×™× × ×©×•×™ ×•×”×©× ×” ×× ×™ ×•××™×©×ª×™ ×¡×•×£ ×¡×•×£ ×™...      m\n",
       "7  ×”×©× ×” ×”×ª×—×œ× ×• ×©×™×¤×•×¥ ×‘×“×™×¨×” ×©×œ× ×• ×‘×ª×œ ××‘×™×‘. ×”×“×™×¨×” ×”...      f"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train set present:\n",
    "df_train.head(8) \n",
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>×›×œ ×§×™×¥ ×× ×™ ×•×”××©×¤×—×” × ×•×¡×¢×™× ×œ××¨×¦×•×ª ×”×‘×¨×™×ª ×œ×•×¡ ×× ×’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>×”×’×¢×ª×™ ×œ×©×™×¨×•×ª ×”××“×™× ×” ××—×¨×™ ×©× ×ª×™×™× ×›×¤×¢×™×œ ×‘×ª× ×•×¢×ª \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>××—×ª ×”××”×‘×•×ª ×”×’×“×•×œ×•×ª ×©×œ×™ ××œ×• ×”×›×œ×‘×™× ×©×œ×™ ×•×©×œ ××™×©×ª...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  ×›×œ ×§×™×¥ ×× ×™ ×•×”××©×¤×—×” × ×•×¡×¢×™× ×œ××¨×¦×•×ª ×”×‘×¨×™×ª ×œ×•×¡ ×× ×’...\n",
       "1                1  ×”×’×¢×ª×™ ×œ×©×™×¨×•×ª ×”××“×™× ×” ××—×¨×™ ×©× ×ª×™×™× ×›×¤×¢×™×œ ×‘×ª× ×•×¢×ª \"...\n",
       "2                2  ××—×ª ×”××”×‘×•×ª ×”×’×“×•×œ×•×ª ×©×œ×™ ××œ×• ×”×›×œ×‘×™× ×©×œ×™ ×•×©×œ ××™×©×ª..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(323, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set present:\n",
    "df_test.head(3)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation:\n",
    "Write your code solution in the following code-cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before We Start - STEPS!\n",
    "### STEP 1: Cleaning the data ğŸ«§\n",
    "### STEP 2: Vectorizing texts ğŸ–¹\n",
    "### STEP 3: Splitting the data âœ‚ï¸\n",
    "### STEP 4: Training the model ğŸƒğŸ½â€â™€ï¸\n",
    "### STEP 5: Score evaluation ğŸ“ˆ\n",
    "### STEP 6: Implementation on the test-set ğŸ‘©ğŸ¼â€ğŸ”¬\n",
    "### STEP 7: Prediction ğŸ”®\n",
    "### STEP 8: Video explanation ğŸ“·\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 - cleaning the data!\n",
    "In this section were going to 'scrub' the data, meanig fixing or removing incorrect values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing spaces, commas, special chars, non hebrew words:\n",
    "def clean_data(df_series):\n",
    "    for item in df_series.index:\n",
    "        # using the regular-exprassion module:\n",
    "        df_series[\"story\"][item] = re.sub(r'[?!@#$%^&:;*\"]', '', df_series[\"story\"][item]) #delete special chars\n",
    "        df_series[\"story\"][item] = re.sub(r'\\.', '', df_series[\"story\"][item]) #delete '.'\n",
    "        df_series[\"story\"][item] = re.sub(r'\\,', '', df_series[\"story\"][item]) #delete ','\n",
    "        df_series[\"story\"][item] = re.sub(r'\\b[a-zA-Z]+\\b', '', df_series[\"story\"][item]) #delete english words\n",
    "        df_series[\"story\"][item] = re.sub(r'\\d+', '', df_series[\"story\"][item]) #delete any number\n",
    "        df_series[\"story\"][item] = re.sub(r\"'\", '', df_series[\"story\"][item]) #delete ' char\n",
    "        df_series[\"story\"][item] = re.sub(r'-', '', df_series[\"story\"][item]) #delete - char\n",
    "\n",
    "    return df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>×›×©×—×‘×¨ ×”×–××™×Ÿ ××•×ª×™ ×œ×—×•×œ ×œ× ×‘×××ª ×—×©×‘×ª×™ ×©×–×” ×™×§×¨×” ×¤...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>×œ×¤× ×™ ×©×”×ª×’×™×™×¡×ª×™ ×œ×¦×‘× ×¢×©×™×ª×™ ×›×œ ×× ×™ ××™×•× ×™× ×œ×™×—×™×“×•...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>×××– ×©×”×ª×—×™×œ×• ×”×œ×™××•×“×™× ×—×œ×•××• ×©×œ ×›×œ ×¡×˜×•×“× ×˜ ×–×” ×”×¤× ...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>×›×©×”×™×™×ª×™ ×™×œ×“ ××˜×•×¡×™× ×”×™×” ×”×“×‘×¨ ×©×”×›×™ ×¨×™×ª×§ ××•×ª×™ ×‘×ª×•...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>â€×”×™×™×ª×™ ××“×¨×™×›×” ×‘×›×¤×¨ × ×•×¢×¨ ×•××ª×× ×”×›×¤×¨ ×”×™×™× ×• ×¦×¨×™×›×™...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>××– ×œ×¤× ×™ ×©× ×” ×‘×“×™×•×§ ×˜×¡×ª×™ ×œ×××¡×˜×¨×“× ×¢× ×©× ×™ ×—×‘×¨×™× ×˜...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>×©×‘×•×¢ ×©×¢×‘×¨ ×”×¢×œ×™×ª×™ ×‘××•×¤×Ÿ ×¡×¤×•× ×˜× ×™ ×¨×¢×™×•×Ÿ ×œ× ×¡×•×¢ ×¢× ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>×œ×¤× ×™ ×—×•×“×© ×¢×‘×¨× ×• ×œ×“×™×¨×” ×‘×‘×™×ª ×©××© ×‘×¢×§×‘×•×ª ××©×¤×—×ª×™ ×”...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>×”×—×•×•×™×” ××•×ª×” ××¨×¦×” ×œ×©×ª×£ ×”×ª×¨×—×©×” ×œ×¤× ×™ ×›××” ×—×•×“×©×™× ×–...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>×¤×¢× ×›×©×”×™×™×ª×™ ×‘×—×• ×œ ×‘×§×‘×•×œ×•××‘×™×” ×›×—×œ×§ ××”×˜×™×•×œ ×©×œ×™ ×œ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story gender\n",
       "0    ×›×©×—×‘×¨ ×”×–××™×Ÿ ××•×ª×™ ×œ×—×•×œ ×œ× ×‘×××ª ×—×©×‘×ª×™ ×©×–×” ×™×§×¨×” ×¤...      m\n",
       "1    ×œ×¤× ×™ ×©×”×ª×’×™×™×¡×ª×™ ×œ×¦×‘× ×¢×©×™×ª×™ ×›×œ ×× ×™ ××™×•× ×™× ×œ×™×—×™×“×•...      m\n",
       "2    ×××– ×©×”×ª×—×™×œ×• ×”×œ×™××•×“×™× ×—×œ×•××• ×©×œ ×›×œ ×¡×˜×•×“× ×˜ ×–×” ×”×¤× ...      f\n",
       "3    ×›×©×”×™×™×ª×™ ×™×œ×“ ××˜×•×¡×™× ×”×™×” ×”×“×‘×¨ ×©×”×›×™ ×¨×™×ª×§ ××•×ª×™ ×‘×ª×•...      m\n",
       "4    â€×”×™×™×ª×™ ××“×¨×™×›×” ×‘×›×¤×¨ × ×•×¢×¨ ×•××ª×× ×”×›×¤×¨ ×”×™×™× ×• ×¦×¨×™×›×™...      f\n",
       "..                                                 ...    ...\n",
       "748  ××– ×œ×¤× ×™ ×©× ×” ×‘×“×™×•×§ ×˜×¡×ª×™ ×œ×××¡×˜×¨×“× ×¢× ×©× ×™ ×—×‘×¨×™× ×˜...      m\n",
       "749  ×©×‘×•×¢ ×©×¢×‘×¨ ×”×¢×œ×™×ª×™ ×‘××•×¤×Ÿ ×¡×¤×•× ×˜× ×™ ×¨×¢×™×•×Ÿ ×œ× ×¡×•×¢ ×¢× ...      m\n",
       "750  ×œ×¤× ×™ ×—×•×“×© ×¢×‘×¨× ×• ×œ×“×™×¨×” ×‘×‘×™×ª ×©××© ×‘×¢×§×‘×•×ª ××©×¤×—×ª×™ ×”...      m\n",
       "751  ×”×—×•×•×™×” ××•×ª×” ××¨×¦×” ×œ×©×ª×£ ×”×ª×¨×—×©×” ×œ×¤× ×™ ×›××” ×—×•×“×©×™× ×–...      f\n",
       "752  ×¤×¢× ×›×©×”×™×™×ª×™ ×‘×—×• ×œ ×‘×§×‘×•×œ×•××‘×™×” ×›×—×œ×§ ××”×˜×™×•×œ ×©×œ×™ ×œ...      m\n",
       "\n",
       "[753 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'×›×©×—×‘×¨ ×”×–××™×Ÿ ××•×ª×™ ×œ×—×•×œ ×œ× ×‘×××ª ×—×©×‘×ª×™ ×©×–×” ×™×§×¨×” ×¤×©×•×˜ ×××¨×ª×™ ×œ×• ×›×Ÿ ×•×ª×™××¨×ª×™ ×œ×¢×¦× ×™ ×©×–×” ×™×ª×‘×˜×œ ××—×¨×™ ×©×‘×•×¢×™×™× ×‘×¢×¨×š ×× ×™ ××§×‘×œ ×˜×œ×¤×•×Ÿ× ××× ×• ×©×•××¢ ××¦××ª×™ ××—×œ×” ××§×•×“×•×ª ×©× ×•×˜×›×œ ×˜×™×™×œ ×‘×”× ×•××– ×”×‘× ×ª×™ ×©×–×” ×”×•×œ×š ×œ×§×¨×•×ª ×”×ª×—×œ×ª×™ ×œ×”×ª××¨×’×Ÿ× ×¢×œ ×“×‘×¨×™× ×¦×™×•×“ ×œ×”×œ×™×›×” ×ª×™×§×™× ×‘×’×“×™× ×—××™× ×›×¡×£ ×•×“×¨×›×•×Ÿ ××¢×•×“×›×Ÿ ×œ××—×¨ ×ª×›× ×•× ×™× × ×¤×’×©× ×• ×‘×©×“×” ×”×•× ×”×‘×™× ×œ×™ ××ª ××—×“ ××”×ª×™×§×™× ×©×œ×• (×›×™ ×œ×™ ××™×Ÿ ×ª×™×§ ×˜×•×‘ ×œ×˜×™×•×œ×™×) ×•×¢×œ×™× ×• ×œ××˜×•×¡ ×œ××™×˜×œ×™×” ×‘×˜×™×¡×” ×¢×¦××” ×œ× ×”×¦×œ×—×ª×™ ×œ×™×©×•×Ÿ ×”×™×” ×™×œ×“ ×§×˜×Ÿ ×©×‘×›×” ×›×œ ×”×“×¨×š ××¢×¦×‘×Ÿ ×›×©×”×’×¢× ×• ×”×œ×›× ×• ×™×©×¨ ×œ×¡×•×›× ×•×ª ×”×©×›×¨×ª ×”×¨×›×‘ ×•×œ×§×—× ×• ××ª ×”×¨×›×‘ ×©×”×–×× ×• ××¨××© ×¡×™×˜×¨×•××œ C ×‘×¦×‘×¢ ×¡×’×•×œ ×›×™ ×–×” ××” ×©× ×©××¨ ×—×¦×™ ×§×¨×× ×• ×œ×” ×¢×œ×™× ×• ×¢×œ ×—×¦×™ ×•×”×ª×—×œ× ×• ××ª ×”××¡×¢ ×œ×›×™×•×•×Ÿ ××’× ×’××¨×“×” ×”×©×¢×” ×”×™×™×ª×”  ×‘×¢×¨×‘ ×§×¦×ª ×§×¨×™×¨ ×‘×—×•×¥ ×—×•×©×š ××•×•×ª ×•××™×Ÿ ×œ× ×• ××•×©×’ ×œ××Ÿ ×× ×—× ×• × ×•×¡×¢×™× (×¨×§ ×¢× ) ×‘×”×ª×—×œ×” ×”×ª×—×œ× ×• ×œ×—×¤×© ××§×•× ×œ×™×©×•×Ÿ ×‘×• ××¦×× ×• ×¢×™×™×¨×” ×¡××•×›×” ×•×”×—×œ×˜× ×• ×œ×œ×›×ª ×œ×©× ×¢×œ ×”×“×¨×š ×¢×¦×¨× ×• ×‘×¤×™×¦×” ×”××™×¦×” ×”×¨××©×•× ×” ×‘××™×˜×œ×™×” ××©× ×”××©×›× ×• ×œ×¢×™×™×¨×” ×¢×¦××” ×•××¦×× ×• ××›×¡× ×™×™×” ×“×™ × ×—××“×” ×©×‘×” ×¢×¦×¨× ×• ×œ×œ×™×œ×” ×‘×‘×•×§×¨ ×©×œ××—×•×¨×ª ×”×•× ××¦× ××¡×œ×•×œ ×˜×™×•×œ ×¢×œ ××—×“ ×”×”×¨×™× ×‘××–×•×¨ × ×¡×¢× ×• ×œ×©× (× ×¡×™×¢×” ×©×œ ×›×©×¢×” ×‘×¢×¨×š) ×”×ª×—×œ× ×• ×œ×¢×œ×•×ª ×¢× ×”×¨×›×‘ ×œ×›×™×•×•×Ÿ ×”××¡×œ×•×œ ×”×“×¨×š ×”×™×™×ª×” × ×•×¤×™×ª ×¢×¦×™× ×•×™×¢×¨ ××›×œ ×›×™×•×•×Ÿ ×¢×“ ×©×‘××™×–×©×”×• ×©×œ×‘ ×”×•× ××‘×§×© ××× ×™ ×œ×¢×¦×•×¨ ×•×œ×—× ×•×ª ××ª ×”×¨×›×‘ ×× ×™ ×©×œ× ×¨××™×ª×™ ×—× ×™×” ××¡×•×“×¨×ª ×•×œ× ××›×™×¨ ××¡×œ×•×œ×™ ×˜×™×•×œ×™× ×›××œ×” ×œ× ×”×‘× ×ª×™ ××” ×”×•× ×¨×•×¦×” ×¢×¦×¨×ª×™ ××ª ×”×¨×›×‘ ×™×¦×× ×• ××× ×• ××¡×ª×›×œ×™× ×œ×›×™×•×•×Ÿ ×”×”×¨ (×‘×“×™×¢×‘×“ ×”×”×¨ ×”×™×” ×‘×’×•×‘×” ×©×œ  ×§×) ×”×•× ××¦×‘×™×¢ ×œ×¨××© ×”×”×¨ ×•××•××¨ ×œ×™ ×¨×•××” ××ª ×”×¦×œ×‘ ×©× ×œ×©× ×× ×—× ×• ××’×™×¢×™× ×××¨×ª×™ ××•×§×™×™ ××™×¤×” ×”×”×ª×—×œ×” ×”×•× ××•××¨ ×œ×™ ×¤×” ×•×”×ª×—×œ× ×• ×œ×œ×›×ª ×œ×›×™×•×•×Ÿ ×¨××© ×”×”×¨ ×‘×”×ª×—×œ×” ×”×¢×œ×™×” ×”×™×™×ª×” ×“×™ ×ª×œ×•×œ×” ×× ×™ ××•×× × ×‘×œ×™ ××¤×—×“ ×’×‘×”×™× ××‘×œ ×›×œ ×”×–××Ÿ ×“×™×‘×¨×ª×™ ××™×ª×• ××” ×”×™×” ×§×•×¨×” ×× ×× ××™×©×”×• × ×•×¤×œ ××” ×¢×•×©×™× ××™×œ ××–×¢×™×§×™× ×¢×–×¨×” (×”×¢×™×™×¨×™×” ×”×›×™ ×§×¨×•×‘×” ×–×” ×©×¢×” × ×¡×™×¢×” ×× ×™ ×‘×œ×™ ×˜×œ×¤×•×Ÿ ×¢× ×¨×©×ª ×•×”×•× ×¢× ×—×¦×™ ×›×•×—) ×”×¡×›×× ×• ×©×× ××—×“ × ×•×¤×œ ×•×–××ª ×¤×¦×™×¢×” ×§×©×” ×¢×•×¦×¨×™× (×›××•×‘×Ÿ) ×•×× ×¡×™× ×œ×¤× ×•×ª ×›××” ×©××¤×©×¨ ×œ× ×§×¨×” ×œ× ×• ×›×œ×•× ×œ×©××—×ª×™ ×œ××—×¨ ×”×˜×™×¤×•×¡ ×©×œ ×©×¢×ª×™×™× ×‘×¢×¨×š×£ ×”×’×¢× ×• ×œ×¨××© ×”×”×¨ ×”× ×•×£ ×”×™×” ××¨×©×™× ×”×©××™×™× ×”×™×• ××›×•×¡×™× ×‘×¢× × ×™× ×•×œ× ×¨××™× ×• ×™×•×ª×¨ ××™×“×™ ×‘×”×ª×—×œ×” ××‘×œ ×œ××—×¨ ×›××” ×“×§×•×ª ×”×©××™×™× ×‘×ª×‘×”×¨×• ×§×¦×ª ×•×¨××™× ×• ×××© ×¨×—×•×§ ××¤×™×œ×• ××ª×¥ ×”××•×˜×• ×©×œ× ×’× ×‘×• ××•×ª×• ×‘×¡×•×£ (×©×–××ª ×”×™×™×ª×” ××—×ª ××”×“××’×•×ª ×©×œ×™) ×œ××—×¨ ××›×Ÿ ×—×–×¨× ×• ×œ××•×˜×• ×•× ×¡×¢× ×• ×¢×™×™×¨×” ×—×“×©×” ×‘××˜×¨×” ×œ×˜×™×™×œ ×©×•×‘ ×•××– ×”×ª×—×™×œ×” ×”×§×•×¨×•× ×” ×œ× ×—×©×‘× ×• ×©×–×” ×™×’×™×¢ ×œ××–×•×¨ ×©×œ× ×• ××‘×œ ×¢×“×™×™×Ÿ ×›×©×©××¢× ×• ×¢×œ ×–×” ×‘×¤×¢× ×”×¨××©×•× ×” ×§×¦×ª ×¦×—×§× ×• ×•×××¨× ×• ×©××™×Ÿ ×¡×™×›×•×™ ×©×–×” ×™×’×™×¢ ××œ×™× ×• ××‘×œ ×›×©×”×’×™×¢ ××œ×™×˜×œ×™×” ××¦×× ×• ××ª ×¢×¦×× ×• ×× ×¡×™× ×œ×—×–×•×¨ ×›××” ×©×™×•×ª×¨ × ××”×¨ ×”×‘×™×ª×” ×‘×¡×•×£ ×”×¡×¤×§× ×• ×œ×¢×œ×•×ª ××ª ×”×˜×™×¡×” ×”×¨××©×˜×” ×××™×˜×œ×™×” ×œ×¤× ×™ ×©×”×™× × ×¡×’×¨×” ×”××–×œ ×”×™×” ×©×”×™×™× ×• ×¢× ××¡×™×›×•×ª ×›×‘×¨ ××– ×œ× × ×“×‘×§× ×•'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'×‘×©× ×” ×”××—×¨×•× ×” ×œ××¨×•×ª ×©×œ× ×”×™×• ×™×•×ª×¨ ××™×“×™×™ ×“×‘×¨×™× ×œ×¢×©×•×ª ×›×™ ×§×•×¨×•× ×” ×•×–×” ×“×•×•×§× ×”×¦×œ×—×ª×™ ×œ×”×•×¦×™× ××©×”×• ×˜×•×‘ ×‘×™×Ÿ ×”×¡×’×¨×™× × ×¡×¢× ×• ×¢× ×—×‘×¨×™× ×œ×˜×™×•×œ ×§××¤×™× ×’ ×‘× ×”×¨ ×”×™×¨×“×Ÿ ×©×œ ×©×œ×•×©×” ×™××™× ×”×ª××¨×’× ×• ×•×§× ×™× ×• ×”×›×œ ××¨××© ×›××•×‘×Ÿ ×©×‘×¢×™×§×¨ ×—×˜×™×¤×™× ×›×™ ×›×œ ×©××¨ ×”×“×‘×¨×™× ×œ× ×‘×××ª ××¢× ×™×™× ×™× ×”×™×” ×“×™×™ ×§×¤×•× ××‘×œ ×‘×›×œ ×–××ª ×”×—×œ×˜× ×• ×œ×”×™×›× ×¡ ×œ××™× ×–××ª ×”×™×™×ª×” ×˜×¢×•×ª ×§×©×” ×›×™ ×—×œ×§ ×××™×ª× ×• ×—×˜×¤×• ×¦×™× ×•×Ÿ ×“×™×™ ×§×©×•×— ×‘×¨×’×¢ ×©×—×–×¨× ×• ×”×‘×™×ª×” ×‘×“×š ×›×œ×œ ×× ×™ ×œ× ××•×”×‘×ª ××ª ×›×œ ×”×¢×™× ×™×™×Ÿ ×”×–×” ×©×œ ×œ×™×©×•×Ÿ ×‘×—×•×¥ ×•×—×¨×§×™× ×•× ××œ×™× ×‘×›×œ ××§×•× ××‘×œ ××ª ×”×××ª ×’×™×œ×™×ª×™ ×©×›×©×¢×•×©×™× ××ª ×–×” ×¢× ×”×× ×©×™× ×”× ×›×•× ×™× ××– ××¤×©×¨ ××¤×™×œ×• ×œ×”× ×•×ª ××–×‘ ×§×¦×ª ×”×ª×—×œ× ×• ××ª ×”×˜×™×•×œ ×¢× ××¡×œ×•×œ ×”×œ×™×›×” ×§×¦×¨ ×œ×œ×›×ª ×–×” ×œ× ×›×™×£ ××– ×¤×—×•×ª × ×”× ×ª×™ ×‘×©×œ×‘ ×”×–×” ××—×¨ ×›×š ××¦×× ×• ××§×•× ×™×—×¡×™×ª × ×•×¨×××œ×™ ×©××¤×©×¨ ×œ×”×§×™× ×‘×• ××ª ×”××•×”×œ ×¡×™×“×¨× ×• ×”×›×œ ×•×”×ª×—×œ× ×• ×œ×©×‘×ª ×•×¡×ª× ×œ×“×‘×¨ ×©×—×›× ×• ×œ×¨×’×¢ ××›×œ ×”×˜×œ×¤×•× ×™× ×•×”×™×™× ×• ×¨×§ ××—×“ ×¢× ×”×©× ×™ ×›×•×œ× ×• ×›×‘×¨ ×”×¡×¤×§× ×• ×œ×©×›×•×— ××” ×–×” ×œ×”×™×•×ª ×‘×œ×™ ×”×˜×œ×¤×•×Ÿ ×œ××©×š ×™×•×ª×¨ ××©×¢×” ×•×”×˜×™×•×œ ×”×–×” ×”×–×›×™×¨ ×œ× ×• ×›××” ×œ×—×‘×•×¨×” ×”×–××ª ×™×© ××–×œ ×©×™×© ×œ× ×• ××—×“ ××ª ×”×©× ×™ ×›××•×‘×Ÿ ×©×©×ª×™× ×• ×•×¦×—×§× ×• ×—×–×¨× ×• ×œ×™×œ×“×•×ª ×•×©×™×—×§× ×• ×‘×›×œ ××™× ×™ ××©×—×§×™ ×§×•×¤×¡× ×©×“××’× ×• ×œ×”×‘×™× ××¨××© ×•×”×¨×’×©× ×• ×‘×××ª ×¢×•×“ ×¤×¢× ×›××• ×™×œ×“×™× ×§×˜× ×™× ×¢×•×“ ×œ×¤× ×™ ×©×”×™×™×ª×” ×›×œ ×”×˜×›× ×•×œ×•×’×™×” ×©×œ ×”×˜×œ×¤×•× ×™× ×•×œ×¤× ×™ ×©×›×•×œ× ×”×™×• ×“×‘×•×§×™× ×œ××¡×›×™× ×”×œ×›× ×• ×œ×™×©×•×Ÿ ×¡×•×¤×¨ ×××•×—×¨ ×‘×œ×™×œ×” ×•×œ××—×¨×ª ×›×©×§×× ×• ×”×›× ×• ×œ×¢×¦×× ×• ××¨×•×—×ª ×‘×•×§×¨ ××˜×•×¨×¤×ª × ×›× ×¡× ×• ×’× ×œ×”×ª×¨×¢× ×Ÿ ×‘××™× ×›××•×‘×Ÿ ×œ××¨×•×ª ×©×”×™×” ×××© ×××© ×§×¨ ×”×—×œ×˜× ×• ×œ××¡×•×£ ××ª ×”×“×‘×¨×™× ×©×œ× ×• ×•×œ× ×¡×•×ª ×œ××¦×•× ××¡×¢×“×” ×˜×•×‘×” ×‘××™×–×•×¨ ×›×™ ×›×‘×¨ × ×”×™×™× ×• ×¨×¢×‘×™× ×•×“×™×™ ×¡×™×™×× ×• ××ª ×›×œ ××” ×©×”×‘×× ×• ×œ××›×•×œ ×™×•× ×œ×¤× ×™ ×‘×’×œ×œ ×©×”×™×™× ×• ×§×¦×ª ×©×ª×•×™×™× ×•××›×œ× ×• ×¤×©×•×˜ ×‘×œ×™ ×”×¤×¡×§×” ××¦×× ×• ××¡×¢×“×” ×××© ×—××•×“×” ×•×”×ª×™×™×©×‘× ×• ×©× ×œ××›×•×œ ×¦×”×¨×™×™× ××›×œ× ×• ×××© ×”×¨×‘×” ×™×•×ª×¨ ××™×“×™×™ ×•×›× ×¨××” ×”×™×™× ×• ×¦×¨×™×›×™× ×‘×¢×¨×š ×—×¦×™ ××”×›××•×ª ×©×”×–×× ×• ××‘×œ ×“×•×•×§× ×™×¦× ×˜×•×‘ ×›×™ ×™×›×œ× ×• ×œ×§×—×ª ××ª ××” ×©× ×©××¨ ×‘×˜×™×™×§ ××•×•××™ ×•×›×›×” ×‘×¢×¦× ×™×”×™×” ×œ× ×• ×›×‘×¨ ××•×›×œ ××•×›×Ÿ ×œ××›×•×œ ×‘×¢×¨×‘ ×”×©× ×™ ×©×œ ×”×§××¤×™× ×’ ×©×œ× ×• ×—×–×¨× ×• ×œ××•×ª×• ××§×•× ×©×”×™×™× ×• ×‘×• ×•×”×§×× ×• ×¢×•×“ ×”×¤×¢× ××ª ×”××•×”×œ×™× ×‘×™×œ×™× ×• ××ª ××•×ª×• ×”×¢×¨×‘ ×‘×“×™×•×§ ×›××• ×”×¢×¨×‘ ×”×§×•×“× ×•×‘×•×§×¨ ××—×¨×™ ×›×‘×¨ ×”×ª×§×¤×œ× ×• ×•×—×–×¨× ×• ×”×‘×™×ª×”'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'×‘×ª×—×™×œ×ª ×”×§×•×¨× ×” ×‘××–×•×¨ ×”×¡×’×¨ ×”×¨××©×•×Ÿ ×× ×™ ×•×‘×ª ×”×–×•×’ ×©×œ×™ ×”×™×™× ×• ×××•×¨×™× ×œ×—×’×•×’  ×©× ×™× ×©×œ ×–×•×’×™×•×ª ××‘×œ ××›×™×•×•×Ÿ ×©×”×›×œ ×”×™×” ×¡×’×•×¨ ×•×œ× ×”×™×” ×”×¨×‘×” ××” ×œ×¢×©×•×ª ××– ×—×©×‘× ×• ×©×›×œ ××—×“ ×™×¢×©×” ××©×”×• × ×—××“ ×œ×©× ×™ ×¨×§ ×›×“×™ ×©× ×•×›×œ ×œ×¦×™×™×Ÿ ××ª ×”×™×•× ×”×–×” ×›×™×•× ××™×•×—×“ ×¢×‘×•×¨× ×• ×‘×ª ×”×–×•×’ ×©×œ×™ ×‘×—×¨×” ×œ×¢×©×•×ª ××©×”×• ×‘××•×ª×• ×”×™×•× ×•×× ×™ ×”×™×™×ª×™ ×××•×¨ ×œ×¢×©×•×ª ×™×•× ××—×¨×™×” ×›××©×¨ ×”×’×™×¢ ×”×™×•× ×”×™× ×‘×™×§×©×” ××× ×™ ×œ×—×–×•×¨ ×”×‘×™×ª×” ×§×¦×ª ×™×•×ª×¨ ×××•×—×¨ ×›×“×™ ×©×ª×•×›×œ ×œ××¨×’×Ÿ ××ª ×›×œ ×”×“×‘×¨×™× ×©×¨×¦×ª×” ×›×©××¨ ×”×’×¢×ª×™ ×”×‘×™×ª×” ×‘×¡×‘×™×‘×•×ª  ×‘×¢×¨×‘ ×”×™× ×‘×™×§×©×” ×©×××¡×•×£ ××•×ª×” ×•× ×™×¡×¢ ×œ×—×•×£ ×”×™× ×›×©××¨ ×”×’×¢× ×• ×œ×—×•×£ ×”×™× ×”×™× ×¤×ª×—×” ×©××™×›×” ×›×œ ×”×—×•×£ ×•×¢×©×” ×œ×™ ××Ÿ ×¤×™×§× ×™×§ ×‘×™× ×‘×¢×¨×‘ ×”×™× ×”×‘×™××” ×‘×§×‘×•×§ ×™×™×Ÿ ×•××•×›×œ ×©×”×™× ×”×›×™× ×” ×œ×‘×“ ×•×‘× ×•×¡×£ ×”×›×™× ×” ×’× ×§×™× ×•×— ×©×•×§×•×œ×“ ××™×•×—×“ ×”×™×” ×××•×“ × ×—××“ ×‘××•×ª×• ×”×™×•× ×•×©× ×™× ×• × ×”× ×™× ×• ×›×©××¨ ×”×’×™×¢ ×”×™×•× ×œ××—×¨×ª ×”×™× ×¢×‘×“×” ×¢×“ ×©×¢×” ×××•×—×¨×ª ×•××– ×›×‘×¨ ×™×“×¢×ª×™ ××¨××© ×©×× ×™ ××›×™×Ÿ ×œ×” ×”××‘×•×¨×’×¨ ×›××• ×©×”×™× ××•×”×‘×ª ×•×’× ×“×™×‘×¨× ×• ×¢×œ ×–×” ×”×¨×‘×” ×–××Ÿ ×©×‘× ×œ× ×• ×œ××›×•×œ ×”××‘×•×¨×’×¨ ×˜×•×‘ ××– ×™×¦××ª×™ ××•×§×“× ××”×¢×‘×•×“×” ×•×”×œ×›×ª×™ ×œ×§×‘×¦×™×” ×§× ×™×™×ª×™ ×‘×©×¨ ×˜×—×•×Ÿ ×× ×ª×—×™× ×˜×•×‘×™× ×××•×“ ×”×’×¢×ª×™ ×”×‘×™×ª×” ×•×”×ª×—×™×œ×ª×™ ×œ×”×›×™×Ÿ ×”×›×œ ×›×©××¨ × ×›× ×¡×” ×”×‘×™×ª×” ×›×‘×¨ ×”×™×• × ×¨×•×ª ×“×•×§×œ×™× ×•××•×•×™×¨×” ×¨×•×× ×˜×™×ª ×‘×™×§×©×ª×™ ××× ×” ×©×ª×©×‘ ×©×‘×©×•×œ×—×Ÿ ×•××– ××™×™×“ ×”×’×©×ª×™ ×œ×” ××ª ×”××•×›×œ ×•×›×œ ×”×“×‘×¨×™× ××¡×‘×™×‘ ××›×œ× ×• ×”×›×œ ×”×™× ×××¨×” ×©×”×™×” ×œ×” ×××•×“ ×˜×¢×™× ×•× ×”× ×ª×” ×××•×“ ×™×•× ×œ××—×¨ ××›×Ÿ ×× ×™ ××§×‘×œ ×©×™×—×” ××× ×” ×©×”×™× × ×•×¡×¢×ª ×œ×”×•×¨×™× ×©×œ×” ×•×”×™× ×œ× ××ª×›×•×•× ×ª ×œ×—×–×•×¨ ×•×©×”×™× ×¨×•×¦×¦×” ×œ×”×™×¤×¨×“ ×× ×™ ×”×™×™×¥×™ ××•×¤×ª×¢ ×××•×“ ××”×“×‘×¨ ×‘×¢×™×§×¨ ×›×©×™×•× ×§×•×“× ×”×›×œ ×”×™×” ×‘×¡×“×¨ ×•×–×” ×¤×ª××•× ××’×™×¢ ××©×•× ××§×•× ×”×™× × ×™×ª×§×” ××ª ×”×©×™×—×” ×•×œ× ×¢× ×ª×” ×œ×™ ×‘××©×š ×›××” ×©×¢×•×ª ×¢×“ ×©×”×—×œ×˜×ª×™ ×œ× ×¡×•×¢ ×œ×‘×™×ª ×”×•×¨×™×” ×•×œ×”×‘×™×Ÿ ××” ×”×¡×™×¤×•×¨ ×•××” ×§×¨×” ×›××©×¨ ×”×’×¢×ª×™ ×œ×©× ×”×ª×—×œ× ×• ×œ×“×‘×¨ ×œ××¨×•×ª ×©×”×™× ×××© ×œ× ×”×™×™×ª×” ××¢×•× ×™×™× ×ª ××‘×œ ×”×ª×¢×§×©×ª×™ ××™×ª×” ×•×œ×‘×•×¡×£ ×”×™× ×”×¡×›×™××” ××¡×ª×‘×¨ ×©×›×©×”×™× ×¡×™×¤×¨×” ×œ×”×•×¨×™× ×©×œ×” ×¢×œ ××” ×¢×©×™×ª×™ ×œ×” ×”× ×¨×§ ×××¨×• ×“×‘×‘×¨×™× ×¨×¢×™× ×•×©×œ× ××’×™×¢ ×œ×” ×•×”×™× ×¦×¨×™×›×” ××™×©×”×• ×©×™×§×— ××•×ª×” ×œ×—×•×œ ×•×™×©×œ× ×¢×œ×™×” ×•×œ× ×¨×§ ×©×™×¢×©×” ×œ×” ×“×‘×¨×™× ×›××œ×” ×œ×‘×¡×•×£ ×”×™× ×”×™×™×ª×” ×™×•×ª×¨ ××“×™ × ×ª×•× ×” ×œ×”×©×¤×¢×ª ×”×”×•×¨×™× ×©×œ×” ×•×¨×§ ××—×¨×™ ××¡×¤×¨ ×™××™× ×”×‘×™× ×” ×©×–×• ×˜×¢×•×ª ×”×™× ×—×–×¨×” ×”×‘×™×ª×” ×•×‘×™×§×©×” ×¡×œ×™×—×” ×¢×œ ×”×¢× ×™×™×Ÿ ×•×¢×œ ××™×š ×©×™×¦×'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my check:\n",
    "clean_data(new_df_train)\n",
    "new_df_train.story[0]\n",
    "new_df_train.story[60]\n",
    "new_df_train.story[700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see above, our data is cleaner...Now we can create a new train and test set with the cleaned data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train = clean_data(new_df_train)\n",
    "new_df_test = clean_data(new_df_test)\n",
    "\n",
    "#-------------copy-----------\n",
    "new_df_train_cp = clean_data(new_df_train).copy()\n",
    "new_df_test_cp = clean_data(new_df_test).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 - splitting the data!\n",
    "In this section were going to split the train set to 2 parts:\n",
    "1. test (0.2)\n",
    "2. train (0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586    ×œ××—×¨ ×”×©×—×¨×•×¨ ××”×¦×‘× ×‘×©× ×” ×”××—×¨×•× ×” × ×¤×œ×ª×™ ×œ×‘×•×¨ ×©×œ ×¨...\n",
       "131    ×‘×™×•× ×©×™×©×™ ×”××—×¨×•×Ÿ ×”×ª×§×©×¨ ××œ×™ ×—×‘×¨×™ ×”×˜×•×‘ ×©× ×‘×“×•×™ ×...\n",
       "44     ×œ×¤× ×™ ××¡×¤×¨ ×©× ×™× ×‘×”×™×•×ª×™ ×ª×œ××™×“ ×ª×™×›×•×Ÿ ×”×•×˜×œ×” ×¢×œ×™× ×• ...\n",
       "70     ×× ×—× ×• ×”×›×¨× ×• ×œ×¤× ×™  ×©× ×™× ×‘×ª×§×•×¤×ª ×”×¦×‘× ×”×›×¨× ×• ×“×¨×š ×”...\n",
       "581    ×”×’×¢×ª×™ ×œ×™×•× ×¡×™×™×¨×•×ª ×‘ ×•×—×™×›×™×ª×™ ×‘×˜×•×¨ ×©×‘×™×§×©×• ×œ×¢××•×“ ...\n",
       "Name: story, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "586    m\n",
       "131    m\n",
       "44     m\n",
       "70     f\n",
       "581    m\n",
       "Name: gender, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "716    ×™×•× ××—×“ ×‘××”×œ×š ×—×•×¤×©×ª ×—× ×•×›×” ×”×œ×›× ×• ×›××” ×—×‘×¨×™× ×œ×©×—×§...\n",
       "651    ×œ×¤× ×™ ×›×—×¦×™ ×©× ×” ×¢×‘×¨×ª×™ ×œ×’×•×¨ ×‘×¦×¤×•×Ÿ ×¢× ×‘×ª ×–×•×’×ª×™ ×¢×‘×¨...\n",
       "371    ×›×©×”×ª×—×™×œ×” ×”×§×•×¨×•× ×” ×”×œ×™××•×“×™× ×¢×‘×¨×• ×œ×”×™×•×ª ×¨×§ ×‘×–×•× ×›...\n",
       "77     ×‘×©× ×” ×”××—×¨×•× ×” ×¢×‘×¨×ª×™ ×“×™×¨×” ×œ×¢×™×¨ ×©×™×© ×‘×” ×™× ×›×œ ×—×™×™ ...\n",
       "212    ×œ×¤× ×™ ×—×¦×™ ×©× ×” ×”×™×™×ª×™ ×‘××™×˜×œ×™×” ×¢× ×”××©×¤×—×” ×©×œ×™ ×–××ª ×”...\n",
       "Name: story, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "716    m\n",
       "651    m\n",
       "371    f\n",
       "77     f\n",
       "212    f\n",
       "Name: gender, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function will split the data and determain:\n",
    "# df[\"story\"] - input features (X)\n",
    "# df[\"gender\"] - labels (y)\n",
    "\n",
    "def splitting_data(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"story\"], df[\"gender\"], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#calling the fun:\n",
    "X_train, X_test, y_train, y_test = splitting_data(new_df_train.copy())\n",
    "X_train_cp, X_test_cp, y_train_cp, y_test_cp = splitting_data(new_df_train.copy())\n",
    "\n",
    "#presenting the data:\n",
    "#train:\n",
    "X_train_cp.head(5)\n",
    "y_train_cp.head(5)\n",
    "#test:\n",
    "X_test_cp.head(5)\n",
    "y_test_cp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 - vectorizing texts!\n",
    "In this section were going to convert out text data to numerical vectors:\n",
    "male - 0\n",
    "female - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we created 2 new columns for female & male: if a text was written by male it would get 1 in the 'gender_m' column:\n",
    "#we changed the strings 'f' & 'm' to numeric values:\n",
    "new_train_cp = pd.get_dummies(new_df_train.copy(), columns=['gender'], prefix=['gender'])\n",
    "\n",
    "#Initialize the TF-IDF vectorizer:\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Fit and transform the training data:\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_cp)\n",
    "\n",
    "#Transform the testing data:\n",
    "X_test_tfidf = vectorizer.transform(X_test_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 - training the model!\n",
    "In this section were going to train our model based on a few algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "### This module works effectively on binary classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#'?_train_tfidf_copy' writing format represent LR model\n",
    "\n",
    "X_train_tfidf_copy = X_train_tfidf.copy()\n",
    "y_train_copy = y_train_cp.copy()\n",
    "X_test_tfidf_copy = X_test_tfidf.copy()\n",
    "y_test_LG = y_test_cp.copy()\n",
    "\n",
    "#Initialize the logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "#Train the model on the training data\n",
    "logreg_model.fit(X_train_tfidf_copy, y_train_copy)\n",
    "\n",
    "#Make predictions on the testing data\n",
    "logreg_predictions = logreg_model.predict(X_test_tfidf_copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier: can handle both classification and regression tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#'?_train_tfidf_forest' writing format represent RF model\n",
    "\n",
    "X_train_tfidf_forest = X_train_tfidf.copy()\n",
    "y_train_forest = y_train_cp.copy()\n",
    "X_test_tfidf_forest = X_test_tfidf.copy()\n",
    "y_test_cp_forest = y_test_cp.copy()\n",
    "\n",
    "#Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "#Train the model on the training data\n",
    "rf_classifier.fit(X_train_tfidf_forest, y_train_forest)\n",
    "\n",
    "#Make predictions on the testing data\n",
    "rf_predictions = rf_classifier.predict(X_test_tfidf_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier:  builds multiple weak learners sequentially and focusing on correcting the errors: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#'?_train_tfidf_GBC' writing format represent GBC model\n",
    "\n",
    "X_train_tfidf_GBC = X_train_tfidf.copy()\n",
    "y_train_GBC = y_train_cp.copy()\n",
    "X_test_tfidf_GBC = X_test_tfidf.copy()\n",
    "y_test_cp_GBC = y_test_cp.copy()\n",
    "\n",
    "#Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "#Train the model on the training data\n",
    "gb_classifier.fit(X_train_tfidf_GBC, y_train_GBC)\n",
    "\n",
    "#Make predictions on the testing data\n",
    "gb_predictions = gb_classifier.predict(X_test_tfidf_GBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) Classifier: KNN is a simple algorithm that classifies an object by the majority class of its k nearest neighbors in the training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#'?_train_tfidf_KNN' writing format represent KNN model\n",
    "\n",
    "X_train_tfidf_KNN = X_train_tfidf.copy()\n",
    "y_train_KNN = y_train_cp.copy()\n",
    "X_test_tfidf_KNN = X_test_tfidf.copy()\n",
    "y_test_cp_KNN = y_test_cp.copy()\n",
    "\n",
    "#Initialize the KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "#Train the model on the training data\n",
    "knn_classifier.fit(X_train_tfidf_KNN, y_train_KNN)\n",
    "\n",
    "#Make predictions on the testing data\n",
    "knn_predictions = knn_classifier.predict(X_test_tfidf_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: support vector machine - good at solving binary classification problems, which require classifying the elements of a data set into two groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#'?_train_tfidf_SVM' writing format represent SVM model\n",
    "\n",
    "X_train_tfidf_SVM = X_train_tfidf.copy()\n",
    "y_train_SVM = y_train_cp.copy()\n",
    "X_test_tfidf_SVM = X_test_tfidf.copy()\n",
    "y_test_cp_SVM = y_test_cp.copy()\n",
    "\n",
    "#Initialize the SVM Classifier with a linear kernel\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "#Train the model on the training data\n",
    "svm_classifier.fit(X_train_tfidf_SVM, y_train_SVM)\n",
    "\n",
    "#Make predictions on the testing data\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 - score evaluation!\n",
    "In this section were going to evaluate the scores from the previous step - how reliable our models are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression mdel's performance, our target is min 'macro avg' of 0.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.00      0.00      0.00        37\n",
      "           m       0.75      1.00      0.86       114\n",
      "\n",
      "    accuracy                           0.75       151\n",
      "   macro avg       0.38      0.50      0.43       151\n",
      "weighted avg       0.57      0.75      0.65       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test_LG, logreg_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Random Forest Classifier model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.00      0.00      0.00        37\n",
      "           m       0.75      1.00      0.86       114\n",
      "\n",
      "    accuracy                           0.75       151\n",
      "   macro avg       0.38      0.50      0.43       151\n",
      "weighted avg       0.57      0.75      0.65       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Classifier:\")\n",
    "print(classification_report(y_test_cp_forest, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Gradient Boosting Classifier model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.71      0.27      0.39        37\n",
      "           m       0.80      0.96      0.88       114\n",
      "\n",
      "    accuracy                           0.79       151\n",
      "   macro avg       0.76      0.62      0.63       151\n",
      "weighted avg       0.78      0.79      0.76       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Classifier:\")\n",
    "print(classification_report(y_test_cp_GBC, gb_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the K-Nearest Neighbors Classifier model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.42      0.27      0.33        37\n",
      "           m       0.79      0.88      0.83       114\n",
      "\n",
      "    accuracy                           0.73       151\n",
      "   macro avg       0.60      0.57      0.58       151\n",
      "weighted avg       0.70      0.73      0.71       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"K-Nearest Neighbors Classifier:\")\n",
    "print(classification_report(y_test_cp_KNN, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SVM model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier (Linear Kernel):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.00      0.00      0.00        37\n",
      "           m       0.75      1.00      0.86       114\n",
      "\n",
      "    accuracy                           0.75       151\n",
      "   macro avg       0.38      0.50      0.43       151\n",
      "weighted avg       0.57      0.75      0.65       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Classifier (Linear Kernel):\")\n",
    "print(classification_report(y_test_cp_SVM, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, our scores are not higt enongh:\n",
    " LG-0.43 /\n",
    " RF-0.43 /\n",
    " GBC-0.63 /\n",
    " KNN-0.58 /\n",
    " SVM-0.43\n",
    " \n",
    "So now what?\n",
    "we'll start with the KNN model and try to 'play' with it so we can get higher:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorizing once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8173913043478261\n",
      "Macro Average F1-score: 0.6170289855072464\n"
     ]
    }
   ],
   "source": [
    "#'vectorizer2' represent the TF-IDF vectorizing from the second time (first time was after the splitting step)\n",
    "\n",
    "#Create TF-IDF vectorizer with stop words removed\n",
    "vectorizer2 = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "#Fit and transform the training data\n",
    "X_train_tfidf2 = vectorizer2.fit_transform(X_train_cp)\n",
    "\n",
    "#Transform the test data using the same vectorizer\n",
    "X_test_tfidf2 = vectorizer2.transform(X_test_cp)\n",
    "\n",
    "#Initialize and train the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train_tfidf2, y_train_cp)\n",
    "\n",
    "#Make predictions on the test data\n",
    "knn_predictions = knn_classifier.predict(X_test_tfidf2)\n",
    "\n",
    "#Calculate F1-score\n",
    "f1 = f1_score(y_true=y_test_cp, y_pred=knn_predictions, pos_label='m')\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "#Calculate macro average F1-score\n",
    "f1_macro_avg = f1_score(y_true=y_test_cp, y_pred=knn_predictions, average='macro')\n",
    "print(\"Macro Average F1-score:\", f1_macro_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately out score dropped...let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for Male: 0.8582995951417005\n",
      "F1-score for Female: 0.36363636363636365\n",
      "Average F1-score: 0.6109679793890321\n"
     ]
    }
   ],
   "source": [
    "#Initialize the TF-IDF vectorizer\n",
    "vectorizer3 = TfidfVectorizer() #changed to vectorizer3\n",
    "\n",
    "#Fit and transform the training data\n",
    "X_train_tfidf_knn = vectorizer3.fit_transform(X_train_cp)\n",
    "\n",
    "#Transform the testing data\n",
    "X_test_tfidf_knn = vectorizer.transform(X_test_cp)\n",
    "\n",
    "#Initialize the KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "#Fit the KNN classifier on the training data\n",
    "knn_classifier.fit(X_train_tfidf_KNN, y_train_cp)\n",
    "\n",
    "#Make predictions using the KNN model\n",
    "knn_predictions = knn_classifier.predict(X_test_tfidf_knn)\n",
    "\n",
    "#Compute F1-score treating 'male' as positive\n",
    "f1_male = f1_score(y_true=y_test_cp, y_pred=knn_predictions, pos_label='m')\n",
    "print(\"F1-score for Male:\", f1_male)\n",
    "\n",
    "#Compute F1-score treating 'female' as positive\n",
    "f1_female = f1_score(y_true=y_test_cp, y_pred=knn_predictions, pos_label='f')\n",
    "print(\"F1-score for Female:\", f1_female)\n",
    "\n",
    "#Calculate the average F1-score\n",
    "average_f1 = (f1_male + f1_female) / 2\n",
    "print(\"Average F1-score:\", average_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nothing really changed...let's take the GBC algo' and 'play' with it:\n",
    "### Since we found the better hyperparameter, let's run the GBC module and hope we get higher score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(max_features=1000)),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'knn__n_neighbors': [3, 5, 7], 'knn__p': [1, 2],\n",
       "                         'knn__weights': ['uniform', 'distance']},\n",
       "             scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'knn__n_neighbors': 3, 'knn__p': 2, 'knn__weights': 'distance'}\n",
      "Best F1-score: 0.5724442274190631\n",
      "K-Nearest Neighbors Classifier (Best Hyperparameters):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.43      0.41      0.42        37\n",
      "           m       0.81      0.82      0.82       114\n",
      "\n",
      "    accuracy                           0.72       151\n",
      "   macro avg       0.62      0.61      0.62       151\n",
      "weighted avg       0.72      0.72      0.72       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline with TF-IDF vectorizer and KNN classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)), \n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__p': [1, 2],\n",
    "    'knn__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV on training data\n",
    "grid_search.fit(X_train_cp, y_train_cp)\n",
    "\n",
    "# Get the best KNN model with optimized hyperparameters\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best KNN model on test data\n",
    "knn_predictions = best_knn_model.predict(X_test_cp)\n",
    "\n",
    "# Print the best hyperparameters and F1-score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the KNN model's performance\n",
    "print(\"K-Nearest Neighbors Classifier (Best Hyperparameters):\")\n",
    "print(classification_report(y_test_cp, knn_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's a little bit better...let's try harder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the same vectorizing but with the X_train_tfidf2:\n",
    "X_train_tfidf_GBC = X_train_tfidf2.copy()\n",
    "y_train_GBC = y_train_cp.copy()\n",
    "X_test_tfidf_GBC = X_test_tfidf2.copy()\n",
    "y_test_cp_GBC = y_test_cp.copy()\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "gb_classifier.fit(X_train_tfidf_GBC, y_train_GBC)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "gb_predictions = gb_classifier.predict(X_test_tfidf_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.80      0.32      0.46        37\n",
      "           m       0.82      0.97      0.89       114\n",
      "\n",
      "    accuracy                           0.81       151\n",
      "   macro avg       0.81      0.65      0.67       151\n",
      "weighted avg       0.81      0.81      0.78       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Classifier:\")\n",
    "print(classification_report(y_test_cp_GBC, gb_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We improved to 0.67!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 - implementation on the test-set!\n",
    "In this section were going to give our model the 'real' test - would he make it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_features=1000)),\n",
       "                ('knn', KNeighborsClassifier())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['m', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'f', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'f', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'f', 'm', 'm', 'm', 'f', 'f', 'm', 'm',\n",
       "       'f', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'f', 'm', 'm', 'f', 'm', 'f', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'm', 'f', 'm', 'm', 'f', 'm', 'm', 'f', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'f', 'm', 'm', 'm', 'm', 'f', 'm', 'm', 'f',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'f', 'm', 'f', 'f', 'm', 'm', 'm', 'f', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'f', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm',\n",
       "       'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)), #changed TfidfVectorizer to vectorizer2\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# fit\n",
    "#X_train, X_test, y_train, y_test = splitting_data(new_df_train.copy())\n",
    "final_model.fit(X_test, y_test)\n",
    "# predict - 0=m , 1=f\n",
    "pred_test_0_1 = final_model.predict(new_df_test.story)\n",
    "pred_test_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 - prediction!\n",
    "In this section were going to present 10 prediction examples (5 head, 5 tail) from out test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      m\n",
       "1      m\n",
       "2      m\n",
       "3      m\n",
       "4      m\n",
       "      ..\n",
       "318    m\n",
       "319    m\n",
       "320    m\n",
       "321    m\n",
       "322    m\n",
       "Name: pred_catg, Length: 323, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = pd.Series(pred_test_0_1, name='pred_catg').replace({0:'m',1:'f'})\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have come to the end of our journey, all there's left is our video. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8 - video explanation!\n",
    "In this section were going to explain our flow by walking you throght the steps. \n",
    "Link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://youtu.be/PM7vZcRR-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv (optional)\n",
    "After you're done save your output to the 'classification_results.csv' csv file.<br/>\n",
    "We assume that the dataframe with your results contain the following columns:\n",
    "* column 1 (left column): 'test_example_id'  - the same id associated to each of the test stories to be predicted.\n",
    "* column 2 (right column): 'predicted_category' - the predicted gender value for each of the associated story. \n",
    "\n",
    "Assuming your predicted values are in the `df_predicted` dataframe, you should save you're results as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
